<!DOCTYPE html>
<html>

<head>
  <!-- <style>
    body {
      font-family: 'Noto Sans', sans-serif;
      line-height: 1.6;
    }
    .title,
    .subtitle {
      font-family: 'Google Sans', sans-serif;
      margin-top: 10px;
      margin-bottom: 20px;
    }

    .content p {
      margin-bottom: 20px;
    }

    .hero {
      margin-bottom: 40px;
    }

    .publication-title {
      font-size: 2.5rem;
      /* Adjust the title font size */
    }

    .publication-authors {
      margin-bottom: 20px;
    }

    .content {
      font-size: 1.3rem;
      /* Adjust content font size */
    }

    .column.has-text-centered {
      padding: 20px 40px;
    }

    footer {
      padding: 20px 0;
      background-color: #f7f7f7;
      /* A light gray background for the footer for distinction */
    }

    .link-block {
      margin: 0 5px;
      /* Add some space between buttons */
    }
  </style> -->

  <meta charset="utf-8">
  <meta http-equiv='cache-control' content='no-cache'>
  <meta http-equiv='expires' content='0'>
  <meta http-equiv='pragma' content='no-cache'>
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="Octopus">
  <meta property="og:title" content="Octopus (2023)" />
  <meta property="og:description"
    content="Octopus: Learning Embodied Vision-Language Programming from Environmental Feedback" />
  <meta property="og:url" content="Choiszt.github.io" />
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/images/Octopus.png" />
  <meta property="og:image:width" content="1200" />
  <meta property="og:image:height" content="630" />


  <meta name="twitter:title" content="ECoFLaP (2023)">
  <meta name="twitter:description"
    content="ECoFLaP: An Efficient Coarse-to-Fine Model Pruning Approach with Zeroth-Order Gradient">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/unc.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="Model Pruning, Multimodal Learning, Unimodal Learning">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Octopus: Learning Embodied Vision-Language Programming from Environmental Feedback</title>
  <link rel="icon" type="image/x-icon" href="static/images/octopus.png">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>

<body>
  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <img src="static/images/octopus.png" style="width: 450px; height: auto;">
            <h1 class="title is-1 publication-title">Learning Embodied Vision-Language Programming from Environmental
              Feedback</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <p class="author-block">
                <a href="https://github.com/Jingkang50" target="_blank">Jingkang Yang<sup
                    class="superscript">*</sup></a>
                <sup>1</sup>,
                <a href="https://github.com/dongyh20" target="_blank">Yuhao Dong<sup class="superscript">*</sup></a>
                <sup>2,5</sup>,
                <a href="https://github.com/choiszt" target="_blank">Shuai Liu<sup class="superscript">*</sup></a>
                <sup>3,5</sup>,
                <a href="https://github.com/Luodian" target="_blank">Bo Li<sup class="superscript">*</sup></a>
                <sup>1</sup>
                <br>
                Ziyue Wang
                <sup>†,1</sup>,
                Chencheng Jiang
                <sup>†,4</sup>,
                Haoran Tan
                <sup>†,3</sup>,
                Jiamu Kang
                <sup>†,2</sup>
                <br>
                <a href="https://zhangyuanhan-ai.github.io/" target="_blank">Yuanhan Zhang</a>
                <sup>1</sup>,
                <a href="https://kaiyangzhou.github.io/" target="_blank">Kaiyang Zhou</a>
                <sup>1,&#9993</sup>,
                <a href="https://liuziwei7.github.io/" target="_blank">Ziwei Liu</a>
                <sup>1,5,&#9993</sup>
              </p>
            </div>
            <br>
            <div class="is-size-5 publication-authors">
              <span><sup>1</sup>S-Lab, Nanyang Technological
                University</span>&nbsp;&nbsp;&nbsp;&nbsp;<span><sup>2</sup>Tsinghua University</span>
              <br>
              <span><sup>3</sup>Beijing University of Posts and Telecommunications</span>
              <br>
              <span><sup>4</sup>Xi'an Jiaotong University</span>&nbsp;&nbsp;<span><sup>5</sup>Shanghai AI
                Laboratory</span><br><br>
              <span><small><sup>*</sup>Equal Contribution</small></span>&nbsp;&nbsp;
              <span><small><sup>†</sup>Equal Engineering Contribution</small></span>&nbsp;&nbsp;
              <span><small><sup>&#9993</sup>Corresponding Author</small>
            </div>

            <div class="column has-text-centered">
              <div class="publication-links">
                <!-- Arxiv PDF link -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2310.02998" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Paper</span>
                  </a>
                </span>

                <!-- Supplementary PDF link -->
                <!-- <span class="link-block">
                      <a href="static/pdfs/supplementary_material.pdf" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Supplementary</span>
                    </a>
                  </span> -->

                <!-- Github link -->
                <span class="link-block">
                  <a href="https://github.com/dongyh20/Octopus" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
                <!-- <span class="link-block">
                  <a href="https://arxiv.org/abs/<ARXIV PAPER ID>" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span> -->
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>


  <!-- Teaser video-->
  <section class="hero teaser">
    <div class="container is-max-desktop">
      <div class="hero-body">
        <video poster="" id="tree" autoplay controls muted loop height="100%">
          <!-- Your video here -->
          <source src="static/videos/ecoflap_video.mp4" type="video/mp4">
        </video>
        <h2 class="subtitle has-text-centered">
          ECoFLaP is a unified coarse-to-fine approach to first efficiently compute sparsity ratios
          for each layer with zeroth-order gradients, and then prune the model in a layer-wise
          manner with the obtained sparsity.
        </h2>
      </div>
    </div>
  </section>
  <!-- End teaser video -->

  <!-- Paper abstract -->
  <section class="section hero is-light" style="font-size: 16px;">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <p>
              The fusion of vision and language in recent vision-language models (VLMs)
              represents a significant
              advancement in multimodal comprehension and interpretation. Furthermore, when seamlessly integrated into
              an embodied agent, it signifies a crucial stride towards the creation of autonomous and context-aware
              systems capable of formulating plans and executing commands with precision. In this paper, we introduce
              <strong>Octopus</strong>, a novel VLM designed to proficiently decipher an agent's vision and textual task
              objectives and to formulate intricate action sequences and generate executable code. Our design allows the
              agent to adeptly handle a wide spectrum of tasks, ranging from mundane daily chores in simulators to
              sophisticated interactions in complex video games. Octopus is trained by leveraging GPT-4 to control an
              explorative agent to generate training data, i.e., action blueprints and the corresponding executable
              code, within our experimental environment called OctoVerse. We also collect the feedback that allows the
              extra training scheme of <strong>Reinforcement Learning with Environmental Feedback (RLEF)</strong>.
              Through a series of experiments, we illuminate Octopus's functionality and present compelling results, and
              the proposed RLEF turns out to refine the agent’s decision-making. By open-sourcing our model
              architecture, simulator, and dataset, we aspire to ignite further innovation and foster collaborative
              applications within the broader embodied AI community. The codebase is released at
              <a href="https://github.com/dongyh20/Octopus" target="_blank"
                style="color: blue;">https://github.com/dongyh20/Octopus</a>.
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- Image carouse -->

  <!-- Youtube video -->
  <section class="hero is-small is-light" style="background-color: white; font-size: 16px;">
    <div class="hero-body">
      <div class="container">
        <!-- Paper video. -->
        <h2 class="title is-3">Method</h2>
        <div class="columns is-centered has-text-centered">
          <div class="column is-four-fifths">
            <script src="./carousel.js"></script>
            <!-- column demonstration -->
            <section class="hero is-light is-small" style="background-color: white">
              <div class="hero-body">
                <div class="container">
                  <div id="results-carousel" class="carousel results-carousel">
                    <div class="item item-steve">
                      <img src="static/images/resized_teaser.jpg" alt="Description of Image 1">
                      <h2 class="subtitle has-text-centered">
                        <strong>Data Collection Example for "Cook a Bacon" Task.</strong> GPT-4 perceives the
                        environment through the <strong>environmental message</strong> and produces anticipated plans and
                        code in accordance with the detailed <strong>system message</strong>. This code is subsequently
                        executed in the simulator, directing the agent to the subsequent state. For each state, we
                        gather the environmental message, wherein <strong>observed objects</strong> and
                        <strong>relations</strong> are substituted by egocentric images to serve as the training input. The
                        response from GPT-4 acts as the training output. Environmental feedback, specifically the
                        determination of whether each target state is met, is documented for RLEF training.
                      </h2>
                    </div>
                    <div class="item item-chair-tp">
                      <img src="static/images/resized_pipeline.jpg" alt="Description of Image 2">
                      <h2 class="subtitle has-text-centered">
                        <strong>The provided image depicts a comprehensive pipeline for data collection
                          and training.</strong> In the <strong>Data Collection Pipeline</strong>, environmental
                        information is captured, parsed into a scene graph, and combined to generate <strong>environment
                          message</strong> and <strong>system message</strong>. These messages subsequently drive agent
                        control, culminating in executable code. For the <strong>Octopus Training Pipeline</strong>, the
                        agent's vision and code are input to the Octopus model for training using both
                        <strong>SFT</strong> and <strong>RLEF</strong> techniques. The accompanying text emphasizes the
                        importance of a well-structured system message for GPT-4's effective code generation and notes
                        the challenges faced due to errors, underscoring the adaptability of the model in handling a
                        myriad of tasks. In essence, the pipeline offers a holistic approach to agent training, from
                        environment understanding to action execution.
                      </h2>
                    </div>
                  </div>
                </div>
              </div>
            </section>
          </div>
        </div>
      </div>
    </div>
  </section>
  <!-- End youtube video -->

  <!-- Youtube video -->
  <section class="hero is-small is-light">
    <div class="hero-body">
      <div class="container">
        <!-- Paper video. -->
        <h2 class="title is-3">Results</h2>
        <div class="columns is-centered has-text-centered">
          <div class="column is-four-fifths">
            <script src="./carousel.js">
            </script>
            <!-- colomn deomstration -->
            <section class="hero is-light is-small">
              <div class="hero-body">
                <div class="container">
                  <div id="results-carousel" class="carousel results-carousel">
                    <!-- 第一个轮播项 -->
                    <div class="item item-steve">
                      <img src="static/images/results.jpg" alt="Description of Image 1">
                    </div>
                    <!-- 第二个轮播项 -->
                    <div class="item item-chair-tp">
                      <img src="static/images/Qualitative Results.jpg" alt="Description of Image 2">
                    </div>

                  </div>
                </div>
              </div>
            </section>
          </div>
        </div>
      </div>
    </div>
  </section>
  <!-- End youtube video -->





  <!-- Paper poster -->
  <!-- <section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title">Poster</h2>

      <iframe  src="static/pdfs/sample.pdf" width="100%" height="550">
          </iframe>
        
      </div>
    </div>
  </section> -->
  <!--End paper poster -->


  <!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>
        @inproceedings{Sung2023ECoFLaP,
          author = {Yi-Lin Sung, Jaehong Yoon, Mohit Bansal},
          title = {Octopus: Learning Embodied Vision-Language Programming from Environmental Feedback},
          booktitle = {arXiv:2310.02998},
          year = {2023},
        }
      
      </code></pre>
    </div>
  </section>
  <!--End BibTex citation -->


  <footer class="footer">
    <div class="container">
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">

            <p>
              This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template"
                target="_blank">Academic Project Page Template</a> which was adopted from the <a
                href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
              You are free to borrow the of this website, we just ask that you link back to this page in the footer.
              <br> This website is licensed under a <a rel="license"
                href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
                Commons Attribution-ShareAlike 4.0 International License</a>.
            </p>

          </div>
        </div>
      </div>
    </div>
  </footer>

  <!-- Statcounter tracking code -->

  <!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

  <!-- End of Statcounter Code -->

</body>

</html>