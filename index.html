<!DOCTYPE html>
<html>

<head>
  <style>
    body {
      font-family: 'Noto Sans', sans-serif;
      line-height: 1.6;
    }
    .title,
    .subtitle {
      font-family: 'Google Sans', sans-serif;
      margin-top: 10px;
      margin-bottom: 20px;
    }

    .content p {
      margin-bottom: 20px;
    }

    .hero {
      margin-bottom: 40px;
    }

    .publication-title {
      font-size: 2.5rem;
      /* Adjust the title font size */
    }

    .publication-authors {
      margin-bottom: 20px;
    }

    .content {
      font-size: 1.3rem;
      /* Adjust content font size */
    }

    .column.has-text-centered {
      padding: 20px 40px;
    }

    footer {
      padding: 20px 0;
      background-color: #f7f7f7;
      /* A light gray background for the footer for distinction */
    }

    .link-block {
      margin: 0 5px;
      /* Add some space between buttons */
    }
  </style>

  <meta charset="utf-8">
  <meta http-equiv='cache-control' content='no-cache'>
  <meta http-equiv='expires' content='0'>
  <meta http-equiv='pragma' content='no-cache'>
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="Octopus">
  <meta property="og:title" content="Octopus (2023)" />
  <meta property="og:description"
    content="Octopus: Learning Embodied Vision-Language Programming from Environmental Feedback" />
  <meta property="og:url" content="Choiszt.github.io" />
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/images/Octopus.png" />
  <meta property="og:image:width" content="1200" />
  <meta property="og:image:height" content="630" />


  <meta name="twitter:title" content="ECoFLaP (2023)">
  <meta name="twitter:description"
    content="ECoFLaP: An Efficient Coarse-to-Fine Model Pruning Approach with Zeroth-Order Gradient">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/unc.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="Model Pruning, Multimodal Learning, Unimodal Learning">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Octopus: Learning Embodied Vision-Language Programming from Environmental Feedback</title>
  <link rel="icon" type="image/x-icon" href="static/images/octopus.png">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>

<body>
  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <img src="static/images/octopus.png" style="width: 450px; height: auto;">
            <h1 class="title is-1 publication-title">Learning Embodied Vision-Language Programming from Environmental
              Feedback</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <p class="author-block">
                <a href="https://github.com/Jingkang50" target="_blank">Jingkang Yang<sup
                    class="superscript">*</sup></a>
                <sup>1</sup>,
                <a href="https://github.com/dongyh20" target="_blank">Yuhao Dong<sup class="superscript">*</sup></a>
                <sup>2,5</sup>,
                <a href="https://github.com/choiszt" target="_blank">Shuai Liu<sup class="superscript">*</sup></a>
                <sup>3,5</sup>,
                <a href="https://github.com/Luodian" target="_blank">Bo Li<sup class="superscript">*</sup></a>
                <sup>1</sup>
                <br>
                Ziyue Wang
                <sup>†,1</sup>,
                Chencheng Jiang
                <sup>†,4</sup>,
                Haoran Tan
                <sup>†,3</sup>,
                Jiamu Kang
                <sup>†,2</sup>
                <br>
                <a href="https://zhangyuanhan-ai.github.io/" target="_blank">Yuanhan Zhang</a>
                <sup>1</sup>,
                <a href="https://kaiyangzhou.github.io/" target="_blank">Kaiyang Zhou</a>
                <sup>1,&#9993</sup>,
                <a href="https://liuziwei7.github.io/" target="_blank">Ziwei Liu</a>
                <sup>1,5,&#9993</sup>
              </p>
            </div>
            <br>
            <div class="is-size-5 publication-authors">
              <span><sup>1</sup>S-Lab, Nanyang Technological
                University</span>&nbsp;&nbsp;&nbsp;&nbsp;<span><sup>2</sup>Tsinghua University</span>
              <br>
              <span><sup>3</sup>Beijing University of Posts and Telecommunications</span>
              <br>
              <span><sup>4</sup>Xi'an Jiaotong University</span>&nbsp;&nbsp;<span><sup>5</sup>Shanghai AI
                Laboratory</span><br><br>
              <span><small><sup>*</sup>Equal Contribution</small></span>&nbsp;&nbsp;
              <span><small><sup>†</sup>Equal Engineering Contribution</small></span>&nbsp;&nbsp;
              <span><small><sup>&#9993</sup>Corresponding Author</small>
            </div>

            <div class="column has-text-centered">
              <div class="publication-links">
                <!-- Arxiv PDF link -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2310.02998" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Paper</span>
                  </a>
                </span>

                <!-- Supplementary PDF link -->
                <!-- <span class="link-block">
                      <a href="static/pdfs/supplementary_material.pdf" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Supplementary</span>
                    </a>
                  </span> -->

                <!-- Github link -->
                <span class="link-block">
                  <a href="https://github.com/dongyh20/Octopus" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
                <!-- <span class="link-block">
                  <a href="https://arxiv.org/abs/<ARXIV PAPER ID>" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span> -->
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>


  <!-- Teaser video-->
  <section class="hero teaser">
    <div class="container is-max-desktop">
      <div class="hero-body">
        <video poster="" id="tree" autoplay controls muted loop height="100%">
          <!-- Your video here -->
          <source src="static/videos/ecoflap_video.mp4" type="video/mp4">
        </video>
        <h2 class="subtitle has-text-centered">
          ECoFLaP is a unified coarse-to-fine approach to first efficiently compute sparsity ratios
          for each layer with zeroth-order gradients, and then prune the model in a layer-wise
          manner with the obtained sparsity.
        </h2>
      </div>
    </div>
  </section>
  <!-- End teaser video -->

  <!-- Paper abstract -->
  <section class="section hero is-light" style="font-size: 14px;">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <p>
              The fusion of vision and language in recent vision-language models (VLMs)
              represents a significant
              advancement in multimodal comprehension and interpretation. Furthermore, when seamlessly integrated into
              an embodied agent, it signifies a crucial stride towards the creation of autonomous and context-aware
              systems capable of formulating plans and executing commands with precision. In this paper, we introduce
              <strong>Octopus</strong>, a novel VLM designed to proficiently decipher an agent's vision and textual task
              objectives and to formulate intricate action sequences and generate executable code. Our design allows the
              agent to adeptly handle a wide spectrum of tasks, ranging from mundane daily chores in simulators to
              sophisticated interactions in complex video games. Octopus is trained by leveraging GPT-4 to control an
              explorative agent to generate training data, i.e., action blueprints and the corresponding executable
              code, within our experimental environment called OctoVerse. We also collect the feedback that allows the
              extra training scheme of <strong>Reinforcement Learning with Environmental Feedback (RLEF)</strong>.
              Through a series of experiments, we illuminate Octopus's functionality and present compelling results, and
              the proposed RLEF turns out to refine the agent’s decision-making. By open-sourcing our model
              architecture, simulator, and dataset, we aspire to ignite further innovation and foster collaborative
              applications within the broader embodied AI community. The codebase is released at
              <a href="https://github.com/dongyh20/Octopus" target="_blank"
                style="color: blue;">https://github.com/dongyh20/Octopus</a>.
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>

<!-- Image carousel -->
<section class="hero is-small">
  <div class="hero-body">
      <div class="container">
          <div id="results-carousel1" class="carousel results-carousel1">
              <div class="item">
                  <!-- Your image here -->
                  <h2 class="title is-3" style="text-align: center;">Overview of FunQA.</h2>
                  <img src="static/images/fig_main.png" alt="MY ALT TEXT" />
                  <h2 class="subtitle">
                      <br>FunQA comprises three subsets of surprising videos: 1) HumorQA, 2) CreativeQA, and 3) MagicQA. Each subset is associated with three common tasks: <em>1) counter-intuitive timestamp localization}, </em><em>2) detailed video description}, and 3) reasoning around counter-intuitiveness</em>                            } (see
                      <b>H1-3</b>,
                      <b>C1-3</b>, and
                      <b>M1-3</b>). Furthermore, we offer higher-level tasks tailored for each video type, such as <em>attributing a fitting and vivid title</em> for HumorQA and CreativeQA (see
                      <b>H4</b>,
                      <b>H4</b>), etc.
                  </h2>
              </div>
              <div class="item">
                  <!-- Your image here -->
                  <h2 class="title is-3" style="text-align: center;">Dataset statistics</h2>
                  <img src="static/images/statics.png" alt="MY ALT TEXT" />
                  <h2 class="subtitle">
                      <br>FunQA consists of three subsets, each corresponding to different video types, and is annotated with free-text QA pairs. The first row displays word clouds representing critical annotations for each subset. The second row
                      provides key dataset statistics, including the number of videos for different splits, video length, and QA pair count for three subsets. In the last row, (g) highlights the high-frequency time span of the answer for localization
                      questions in red, (h) shows the average word count of answers, and (i) presents the percentage of consensus between annotators for the same answer in a sampled set.
                  </h2>
              </div>
          </div>
      </div>
  </div>
</section>
<!-- End image carousel -->



<!-- Image carousel -->
<section class="hero is-small">
  <div class="hero-body">
      <div class="container">
          <div id="results-carousel1" class="carousel results-carousel1">
              <div class="item">
                  <!-- Your image here -->
                  <h2 class="title is-3" style="text-align: center;">Comparison between different benchmarks</h2>
                  <h2 class="subtitle">
                      <br>Compare to other datasets, FunQA revolves around the captivating realm of interesting and counter-intuitive videos. The tasks within FunQA are specifically designed to challenge the vision capabilities of models, requiring
                      strong skills in producing an in-depth description, interpretation, and spatial-temporal reasoning. Here we clarify the abbreviation in the table. <b>Anno.</b>: Annotation; <b>M</b>: Manual, <b>A</b>: Automatic; For Input,
                      <b>V, A, S</b>, and <b>B</b> denote Video, Audio, Subtitle, and Bounding-box; <b>VC</b> means visual-centric, <b>Desc.</b> means Description, <b>Expl</b> for Explanation, <b>Expl</b> for Spatial-temporal Reasoning. For QA Tasks,
                      <b>MC</b> denotes Multiple Choice QA, <b>OE</b> means Open Ended QA, and <b>FT</b> means Free Text QA.
                  </h2>
                  <img src="static/images/table1.png" alt="MY ALT TEXT" />

              </div>
              <div class="item">
                  <!-- Your image here -->
                  <h2 class="title is-3" style="text-align: center;">Main result for baselines</h2>
                  <h2 class="subtitle">
                      <br>The FunQA benchmark consists of four task categories. H1, C1, M1 represent the counter-intuitive timestamp localization task, where IOU is used as the metric. H2, C2, M2 represent the detailed video description task, and
                      H3, C3, M3 represent reasoning around counter-intuitiveness. For the higher-level tasks, H4, C4 involve attributing a fitting and vivid title. The responses for all these tasks in free-text format. We use the following metrics:
                      BLEU-4 / ROUGE-L / CIDEr (shown in the first row) and BLEURT / GPT-4 (shown in the second row) for evaluation. C5 represents scoring the video creativity, and the metric is the Difference between the predicted score and the
                      official score. We tested the caption-based and instruction-based models. Here we clarify the abbreviation in the table. L.M.: GIT_LARGE_MSRVTT; L.V.: GIT_LARGE_VATEX; D.C. means finetuned on Dense Caption; FunQA means
                      finetuned on FunQA.
                  </h2>
                  <img src="static/images/table.png" alt="MY ALT TEXT" />

              </div>
              <div class="item">
                  <!-- Your image here -->
                  <h2 class="title is-3" style="text-align: center;">Comparison of responses from different models</h2>
                  <h2 class="subtitle">
                      <br>Here shows the answers given by VideoChat, Video-ChatGPT, and Otter on HumorQA video. On task H2, H3, VideoChat has the best performance. On task H4, Video-ChatGPT and Otter answer better, which is in line with our experiment
                      result. However, the answers from all models are still far from the ground truth. The descriptions of details and counter-intuitive explanations have numerous shortcomings. For example, Video-ChatGPT added incorrect details
                      to the description, such as "wearing sunglasses", the humorous reason for "throwing ketchup" was wrongly interpreted by VideoChat as "knocking over the ketchup bottle", etc.
                  </h2>
                  <img src="static/images/model_comparison.png" alt="MY ALT TEXT" />

              </div>
          </div>
      </div>
  </div>
</section>

  <!-- Youtube video -->
  <section class="hero is-small is-light">
    <div class="hero-body">
      <div class="container">
        <!-- Paper video. -->
        <h2 class="title is-3">Method</h2>
        <div class="columns is-centered has-text-centered">
          <div class="column is-four-fifths">
            <script src="./carousel.js">
            </script>
            <!-- colomn deomstration -->
            <section class="hero is-light is-small">
              <div class="hero-body">
                <div class="container">
                  <div id="results-carousel" class="carousel results-carousel">
                    <!-- 第一个轮播项 -->
                    <div class="item item-steve">
                      <img src="static/images/Task Example.jpg" alt="Description of Image 1">
                    </div>
                    <!-- 第二个轮播项 -->
                    <div class="item item-chair-tp">
                      <img src="static/images/pipeline.jpg" alt="Description of Image 2">
                    </div>

                  </div>
                </div>
              </div>
            </section>
            <!-- <div class="item">
              <img src="static/images/pipeline.png" alt="MY ALT TEXT" />
              <p>While GPT-4 guides the agent toward task completion, its continual trial-and-error approach does more
                than just collect vision-output pairs. This iterative problem-solving provides a rich set of feedback
                data.</p>
              <p>The automatic annotation of the feedback is twofold, focusing on both step-level and task-level
                judgments. <strong>Step-level judgment</strong> assesses the alignment of post-execution states with
                their target states. For instance, steps color-coded in green signify positive feedback. One can
                visualize the action sequence for task completion as a tree, where
                each node embodies a step (subtask), encapsulating an action code. Accompanying each step is a binary
                value that denotes success or failure, giving preference to the successful branch over its counterpart.
                <strong>Task-level judgment</strong>, on the other hand, gauges the successful execution of the overall
                task. If the task is not completed as intended, every state within that task is labeled as negative.
              </p>
              <p>This collated feedback data serves as a foundation for our Reinforcement Learning with Environmental
                Feedback (RLEF) methodology.
              </p>

            </div> -->

            <!-- <div class="publication-video">
            <iframe src="https://www.youtube.com/embed/JkaxUblCGz0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
          </div> -->
          </div>
        </div>
      </div>
    </div>
  </section>
  <!-- End youtube video -->





  <!-- Video carousel -->
  <!-- <section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">Another Carousel</h2>
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-video1">
          <video poster="" id="video1" autoplay controls muted loop height="100%">
            <source src="static/videos/carousel1.mp4"
            type="video/mp4">
          </video>
        </div>
        <div class="item item-video2">
          <video poster="" id="video2" autoplay controls muted loop height="100%">
            <source src="static/videos/carousel2.mp4"
            type="video/mp4">
          </video>
        </div>
        <div class="item item-video3">
          <video poster="" id="video3" autoplay controls muted loop height="100%">\
            <source src="static/videos/carousel3.mp4"
            type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section> -->
  <!-- End video carousel -->


  <!-- Youtube video -->
  <section class="hero is-small is-light">
    <div class="hero-body">
      <div class="container">
        <!-- Paper video. -->
        <h2 class="title is-3">Results</h2>
        <div class="columns is-centered has-text-centered">
          <div class="column is-four-fifths">
            <script src="./carousel.js">
            </script>
            <!-- colomn deomstration -->
            <section class="hero is-light is-small">
              <div class="hero-body">
                <div class="container">
                  <div id="results-carousel" class="carousel results-carousel">
                    <!-- 第一个轮播项 -->
                    <div class="item item-steve">
                      <img src="static/images/results.jpg" alt="Description of Image 1">
                    </div>
                    <!-- 第二个轮播项 -->
                    <div class="item item-chair-tp">
                      <img src="static/images/Qualitative Results.jpg" alt="Description of Image 2">
                    </div>

                  </div>
                </div>
              </div>
            </section>
            <!-- <div class="publication-video">
              <iframe src="https://www.youtube.com/embed/JkaxUblCGz0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
            </div> -->
          </div>
        </div>
      </div>
    </div>
  </section>
  <!-- End youtube video -->





  <!-- Paper poster -->
  <!-- <section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title">Poster</h2>

      <iframe  src="static/pdfs/sample.pdf" width="100%" height="550">
          </iframe>
        
      </div>
    </div>
  </section> -->
  <!--End paper poster -->


  <!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>
        @inproceedings{Sung2023ECoFLaP,
          author = {Yi-Lin Sung, Jaehong Yoon, Mohit Bansal},
          title = {Octopus: Learning Embodied Vision-Language Programming from Environmental Feedback},
          booktitle = {arXiv:2310.02998},
          year = {2023},
        }
      
      </code></pre>
    </div>
  </section>
  <!--End BibTex citation -->


  <footer class="footer">
    <div class="container">
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">

            <p>
              This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template"
                target="_blank">Academic Project Page Template</a> which was adopted from the <a
                href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
              You are free to borrow the of this website, we just ask that you link back to this page in the footer.
              <br> This website is licensed under a <a rel="license"
                href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
                Commons Attribution-ShareAlike 4.0 International License</a>.
            </p>

          </div>
        </div>
      </div>
    </div>
  </footer>

  <!-- Statcounter tracking code -->

  <!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

  <!-- End of Statcounter Code -->

</body>

</html>